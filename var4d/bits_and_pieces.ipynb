{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd0463e-d5b3-4a8c-affd-c92ae36e5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from var4d_components import Var4D_Components\n",
    "from matplotlib import pyplot as plt\n",
    "from mapping_functions import MapFunctions\n",
    "from scipy import optimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f98950-cdcf-4fa1-bb36-b9d7c227ade3",
   "metadata": {},
   "source": [
    "### Big picture\n",
    "\n",
    "The overall objective is to minimize the cost function\n",
    "\n",
    "$$ J = \\frac{1}{2} (\\mathbf{Hx} - \\mathbf{z})^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{Hx} - \\mathbf{z}) + \\frac{1}{2} (\\mathbf{x}-\\mathbf{x}_0)^\\text{T} \\mathbf{S_x}^{-1} (\\mathbf{x}-\\mathbf{x}_0) $$\n",
    "\n",
    "\"Batch\" methods get the exact minimum by solving the linear equation\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial\\mathbf{x}} = \\mathbf{H}^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{Hx} - \\mathbf{z}) + \\mathbf{S_x}^{-1} (\\mathbf{x}-\\mathbf{x}_0) = 0 $$\n",
    "\n",
    "while variational methods get to an approximate minimum by feeding a minimizer with successive values of $J$ and $\\partial J/\\partial\\mathbf{x}$. Which means, for every value of $\\mathbf{x}$, we need a way to calculate $J$ and $\\partial J\\partial\\mathbf{x}$. However, there is a complication.\n",
    "\n",
    "### Ill-conditioned problems \n",
    "\n",
    "Consider the following function to be minimized.\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_i \\alpha_i x_i^2, \\quad \\alpha_i > 0 $$\n",
    "$$ g(\\mathbf{x}) = \\frac{\\partial f}{\\partial\\mathbf{x}} = 2\\mathbf{\\alpha}^\\text{T} \\mathbf{x} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49889fc2-5c3d-4d47-b37a-1be1d09144fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(vec, prefac):\n",
    "    res = 0.0\n",
    "    for x,p in zip(vec, prefac):\n",
    "        res = res + p*x*x\n",
    "    return res\n",
    "def g(vec, prefac):\n",
    "    res = np.zeros(len(vec), dtype=np.float64)\n",
    "    for i,(x,p) in enumerate(zip(vec,prefac)):\n",
    "        res[i] = 2*p*x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4997dc-7b34-437e-9556-44403e161919",
   "metadata": {},
   "source": [
    "We know this has a global minimum at $\\mathbf{x} = \\mathbf{0}$, so let's try to get that with a canned minimizer, starting from $\\mathbf{x} = \\mathbf{1}$ and $\\alpha_i = 1$ for a vector of length $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3567976f-7de2-4822-8300-bef078386e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_args = dict(method='BFGS', jac=g, options=dict(maxiter=100, disp=False))\n",
    "N = 20\n",
    "x0 = np.ones(N) # initial guess of x\n",
    "alpha = np.ones(N)\n",
    "result = optimize.minimize(f, x0, args=alpha, **optim_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185f8f0c-5daf-40d4-a193-7e6127f143ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.77555756e-16, -3.33066907e-16, -3.33066907e-16, -1.66533454e-16,\n",
       "       -4.44089210e-16, -1.11022302e-16, -1.66533454e-16, -1.11022302e-16,\n",
       "       -1.11022302e-16, -1.11022302e-16,  0.00000000e+00, -1.11022302e-16,\n",
       "        1.11022302e-16,  5.55111512e-17,  0.00000000e+00,  2.77555756e-16,\n",
       "        4.44089210e-16,  4.44089210e-16,  4.99600361e-16,  4.99600361e-16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666a6b6-1084-4520-8798-00b7ba8bfbd7",
   "metadata": {},
   "source": [
    "Now let's make it a little more fun/non-trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cbf8a0-9933-42aa-bd4f-e737567a25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_args = dict(method='BFGS', jac=g, options=dict(maxiter=100, disp=False))\n",
    "N = 20\n",
    "x0 = np.ones(N)\n",
    "powers = np.random.randint(-16, 16, size=N)\n",
    "alpha = 10.0**powers\n",
    "result = optimize.minimize(f, x0, args=alpha, **optim_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e691b797-e150-4503-ba22-ea24c185886c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.26280915e-05, -1.54984390e-02, -9.70128488e-08,  1.82454236e-24,\n",
       "        1.53831300e-06,  1.43347373e-11,  1.86039593e-23,  9.99887037e-01,\n",
       "        5.36415891e-17,  1.85906807e-22, -1.43347707e-11,  7.09180282e-20,\n",
       "        1.86808070e-20,  4.17106073e-06,  6.14084543e-04,  1.53829274e-06,\n",
       "        1.86843451e-20,  9.99988704e-01,  9.99887037e-01,  1.86063315e-21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9388d-1a9c-485b-ad34-4968007a9a7e",
   "metadata": {},
   "source": [
    "__THINK: We all *know* that the correct solution is a vector of zeros, so what happened? Why did the minimizer not succeed?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4d56b-7a7b-454e-8838-8a92002fbf5d",
   "metadata": {},
   "source": [
    "### Preconditioning the inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbb018-2d27-4603-aed2-3b455ec43fce",
   "metadata": {},
   "source": [
    "Instead of solving for $\\mathbf{x}$, we want to solve for a vector for which the cost function is (approximately) equally \"stiff\" in all directions. We do this by first calculating the \"square root\" of the prior covariance\n",
    "\n",
    "$$ \\mathbf{L} = \\sqrt{\\mathbf{S_x}} $$\n",
    "\n",
    "*Matrices don't have unique square roots*, and one possibility that is numerically efficient to compute is the Cholesky decomposition,\n",
    "\n",
    "$$ \\mathbf{S_x} = \\mathbf{L}\\mathbf{L}^\\text{T} $$\n",
    "\n",
    "__THINK: What are the conditions on__ $\\mathbf{S_x}$ __for this to be possible?__\n",
    "\n",
    "Once we compute $\\mathbf{L}$, we define a new vector $\\xi$ as $\\mathbf{x} = \\mathbf{L}\\xi + \\mathbf{x}_0$. In terms of $\\xi$, the cost function and its gradient become\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J &= \\frac{1}{2} (\\mathbf{HL}\\xi +\\mathbf{Hx}_0 - \\mathbf{z})^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{HL}\\xi +\\mathbf{Hx}_0 - \\mathbf{z}) + \\frac{1}{2} \\xi^\\text{T} \\xi \\\\\n",
    "\\frac{\\partial J}{\\partial\\xi} &= \\mathbf{L}^\\text{T}\\mathbf{H}^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{HL}\\xi + \\mathbf{Hx}_0 - \\mathbf{z}) + \\xi\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and we perform the entire optimization in terms of $\\xi$, not $\\mathbf{x}$.\n",
    "\n",
    "__THINK: Why does this make the problem easier to solve, numerically?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47737646-b1c2-42fb-989b-799a7f73697c",
   "metadata": {},
   "source": [
    "### Step 1: Construct $\\mathbf{S_x}$\n",
    "\n",
    "$\\mathbf{S_x}$ is the covariance between prior __errors__, e.g., if the prior flux over region 1 is too high by 30% in January, what does that say about the prior flux in region 2 (next to region 1) in February? What about a region halfway around the world and six months later?\n",
    "\n",
    "We usually model $\\mathbf{S_x}$ as a function of space and time, i.e., $\\mathbf{S_x}(ij) = \\sigma_i \\sigma_j \\mathbf{R}_{ij}$, where $\\sigma_i$ is the (Gaussian) error on flux element $i$ and $\\mathbf{R}_{ij}$ is the correlation between prior errors of flux elements $i$ and $j$. __In this toy example__, we have further simplified $\\mathbf{R}_{ij}$ to be separable in time and space, i.e., $\\mathbf{R}_{ij} = \\mathbf{P}_{ij} \\mathbf{T}_{ij}$ where $\\mathbf{P}$ and $\\mathbf{T}$ and spatial and temporal correlation matrices respectively.\n",
    "\n",
    "Finally, we've made the simplifying assumption that $\\mathbf{P}_{ij} = \\delta_{ij}$ and $\\mathbf{T}_{ij} = e^{-\\left|t_i-t_j\\right|/\\tau}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b624ec-9c51-4918-bd7f-fe0c59652f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "var4d = Var4D_Components('bits_and_pieces', verbose=True, store_intermediate=True) # create an empty instance\n",
    "flux_corr_structure = {'temp_corr': 2.0} # tau = 2 months\n",
    "prior_corr = var4d.setup_corr(**flux_corr_structure)\n",
    "var4d.prior_corr = 0.5*(prior_corr + prior_corr.T) # good idea to make it explicitly symmetric to get rid of potential asymmetries due to floating point rounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324e259-1f82-43f6-9c34-dec54a3af641",
   "metadata": {},
   "source": [
    "For the toy example, we choose $\\sigma_i = \\beta \\times \\left|\\text{prior flux}\\right|$. This is not a great choice for fluxes that can be both positive and negative, but is often the default choice for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1288972b-743e-40ca-826c-405c36a2c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f355d5c4634487b11e1130a5720940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting SiB4 to state vector:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var4d.state_prior = var4d.flux_cons.construct_state_vector_from_sib4() # prior flux will be in lat x lon x time format, need to convert to a vector\n",
    "prior_unc_scale = 0.25 # beta\n",
    "var4d.unc_prior = prior_unc_scale * np.abs(var4d.state_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3107b7-aa22-4472-82a0-2f57411456a2",
   "metadata": {},
   "source": [
    "Now that we have both the correlation matrix $\\mathbf{R}$ and the $\\sigma_i$, we can make $\\mathbf{S_x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49da159-2273-4b0d-a2eb-4e3785139478",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_cov = var4d.setup_cov(var4d.prior_corr, var4d.unc_prior)\n",
    "var4d.prior_cov = 0.5*(prior_cov + prior_cov.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135f045-d106-46e5-a5ab-10e4c96baf46",
   "metadata": {},
   "source": [
    "Finally, construct $\\mathbf{L}$ such that $\\mathbf{S_x} = \\mathbf{L}\\mathbf{L}^\\text{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32b901d-b0ff-4aa2-829d-f7eef81ab707",
   "metadata": {},
   "outputs": [],
   "source": [
    "var4d.L = np.linalg.cholesky(var4d.prior_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c0e30-43df-4b4c-924e-62f3e8906c24",
   "metadata": {},
   "source": [
    "### Step 2: Construct $\\mathbf{S_z}$\n",
    "\n",
    "In this toy example, we use the same \"observation errors\" that were used in the OCO2 MIP. In general one needs to put in considerable thought on how to set this up. While this allows off-diagonal elements (i.e., correlation between observations), it is practically hard to implement/code. Here we will stick to a purely diagonal $\\mathbf{S_z}$. We also have to make the choice of which obs to assimilate. To keep the code simple, we simply set the error of all non-assimilated obs to $10^{36}$ ppm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b31cff-a824-404c-9140-dd39899e0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245 of 1156383 obs will be assimilated\n"
     ]
    }
   ],
   "source": [
    "obs_assim_dict = {'sites': ['mlo', 'spo', 'brw', 'smo']} # just the four NOAA observatories, only flask obs\n",
    "var4d.obs_err = var4d.setup_obs_errors(**obs_assim_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4eefc-55d2-40ef-924f-a84c5a861d98",
   "metadata": {},
   "source": [
    "### Step 3: Make sure you have obs to assimilate\n",
    "\n",
    "In the toy example, we generate the obs from a \"true\" flux, i.e., we assume we know reality. In real flux inversions, these are given to us by our colleagues. For illustrating the steps, we will just construct an obs vector of all 400 ppm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a2a196-aa1a-4f18-9329-5652ec90d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "var4d.obs_vec = 400.0 * np.ones_like(var4d.obs_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba48545-cefd-4abb-92b1-faf0c493934f",
   "metadata": {},
   "source": [
    "### Step 4: Enter the 4DVAR loop\n",
    "\n",
    "In the 4DVAR loop, one goes from a given $\\xi$ to flux ($\\mathbf{x}$), transports that flux ($\\mathbf{Hx}$), compares to obs ($\\mathbf{Hx}-\\mathbf{z}$), multiplies by $\\mathbf{S_z}^{-1}$, then calculates the value of $J$ for this $\\xi$. After that, the adjoint transport operator $\\mathbf{H}^\\text{T}$ and the adjoint preconditioning operator $\\mathbf{L}^\\text{T}$ are used to calculate the gradient $\\partial J/\\partial\\xi$. This gradient and $J$ are fed to an optimizer, which gives us back the next value of $\\xi$. This process repeats until some sort of convergence is reached.\n",
    "\n",
    "#### Convert $\\xi$ to flux\n",
    "\n",
    "To illustrate the problem, let us choose a random vector $\\xi$. In practice, one typically starts from $\\xi = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31906791-0dae-4d59-afce-f86cf6a56a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.random.standard_normal(var4d.state_prior.shape[0])\n",
    "x = var4d.state_prior + np.matmul(var4d.L, xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3b5c3-612a-44d1-bed1-79ed3dae9a59",
   "metadata": {},
   "source": [
    "#### Transport this flux and sample at the observations ($\\mathbf{Hx}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79567846-2da5-4892-a2ac-8a68e227f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Added background in  0.33s\n",
      "  Simulated transport in  0.44s\n"
     ]
    }
   ],
   "source": [
    "var4d.progress_bars = {} # stop code from complaining when I run outside a 4DVAR loop\n",
    "obs = var4d.forward_transport(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2e294-c1da-4a30-ba08-5e6c6b7e8508",
   "metadata": {},
   "source": [
    "#### Compare to obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec981c6d-dbf3-4033-9364-2d575fac54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obs_diff = obs - var4d.obs_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e780d-6c00-413b-9107-394faf9ab55b",
   "metadata": {},
   "source": [
    "#### Calculate $J_\\text{obs}$, where\n",
    "\n",
    "$$\n",
    "J_\\text{obs} = \\frac{1}{2} (\\mathbf{Hx} - \\mathbf{z})^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{Hx} - \\mathbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bfed415-4d22-4ac3-a002-10a093870c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_cost = 0.5 * np.sum((model_obs_diff/var4d.obs_err)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00437b7b-92dc-4046-8355-dcef8db2c968",
   "metadata": {},
   "source": [
    "#### Calculate $J_\\text{bg}$ where \n",
    "\n",
    "$$\n",
    "J_\\text{bg} = \\frac{1}{2} (\\mathbf{x}-\\mathbf{x}_0)^\\text{T} \\mathbf{S_x}^{-1} (\\mathbf{x}-\\mathbf{x}_0)\n",
    "$$\n",
    "\n",
    "and then the total cost $J = J_\\text{obs}+J_\\text{bg}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4416208e-2b6e-4bb6-a9a9-cce46d4efb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_cost = 0.5 * np.sum(xi**2)\n",
    "total_cost = obs_cost + bg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047d9e0-bea6-409b-895d-5c6e928189d1",
   "metadata": {},
   "source": [
    "#### Calculate the adjoint forcing, $\\mathbf{S_z}^{-1} (\\mathbf{Hx} - \\mathbf{z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f77c23-ca55-4519-93e0-4a5e2a49533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_forcing = model_obs_diff/(var4d.obs_err**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c899c9-ba48-412a-b265-e2a852ce5465",
   "metadata": {},
   "source": [
    "#### Propagate this through the adjoint transport operator $\\mathbf{H}^\\text{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e574f90e-8220-4731-9be9-340efa2aa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = var4d.adjoint_transport(adj_forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ea306-9d1c-4eb1-87c1-1a8813d373f5",
   "metadata": {},
   "source": [
    "#### Calculate the gradient $\\partial J/\\partial\\xi$\n",
    "\n",
    "The above us $\\partial J_\\text{obs}/\\partial\\mathbf{x}$. Convert that to $\\partial J_\\text{obs}/\\partial\\xi$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec937d0-3894-4101-9482-620be60f4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_preco = np.matmul(var4d.L.T, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a9911-d827-473a-9e61-9c163252d782",
   "metadata": {},
   "source": [
    "and add the gradient of the \"background\", $\\partial J_\\text{bg}/\\partial\\xi = \\xi$, to get the total gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bfdb3e9-efa3-40ae-a4f4-0595e46a31f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gradient = gradient_preco + xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d48eb-4721-44ea-a09c-4ea049c67bd0",
   "metadata": {},
   "source": [
    "The `total_cost` and `total_gradient` are then fed to an optimizer such as BFGS, which tells us the next value of $\\xi$ for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711f751-5bd2-4268-943a-b9edabcf2f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
